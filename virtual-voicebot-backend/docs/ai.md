# ai モジュール詳細設計（asr / llm / tts）

## 1. 共通方針
- すべて「外部サービスのラッパー」として実装する
- ネットワーク再試行ポリシー、タイムアウトの基本値

## 2. ASR (ai::asr)
- 入力: PCMフォーマット・チャンクサイズ・ストリーミング単位
- 出力: partial/final のどちらを扱うか
- エラー種別と app への通知方法

## 3. LLM (ai::llm)
- 入力: プロンプト構成（履歴＋ユーザ発話＋システムメッセージ）
- 出力: 応答テキスト＋メタ情報（終了フラグ/アクションタグなど）
- トークン数・タイムアウト・リトライのポリシー

## 4. TTS (ai::tts)
- 入力: テキスト + オプション（話者/速度）※MVPでどこまでやるか
- 出力: PCM チャンクのサイズ / ストリーミング方法
- エラー時にどう知らせるか

## 5. app から見た I/F まとめ
- app→ai のリクエスト型一覧
- ai→app のレスポンス/イベント型一覧
- キャンセル（通話終了時に ASR/TTS ストリームをどう閉じるか）

### イベント/リクエスト名ドラフト（voice_bot_flow 連携）
- ASR: `AsrRequest` → `AsrResultPartial` / `AsrResultFinal`
- LLM: `LlmRequest` → `LlmResponse`
- TTS: `TtsRequest` → `TtsAudioChunk`

## 6. 現状の実装と差分メモ
- 現状: WAV 一時ファイルを経由し、HTTP (`/transcribe`, `/audio_query`, `/synthesis`) や AWS SDK を直接呼び出している。
- 予定: app↔ai のチャネル経由で PCM/テキストを渡す形に置き換える（I/F 変更は別タスク）。
- ポリシー（タイムアウト/リトライ/フォールバック）は現状のまま維持し、移行後も踏襲する想定。
